# -*- coding: utf-8 -*-
"""french_to_tamil

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dFnYM646b1OgfbvcnxoHIK2HyIV5Fv6O
"""

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Load dataset
french_words = []
tamil_words = []

with open("/content/french_tamil_5letters.txt", "r", encoding="utf-8") as file:
    for line in file:
        if "→" in line:
            french, tamil = line.strip().split("→")
            french, tamil = french.strip().lower(), tamil.strip()
            if len(french) == 5:
                french_words.append(french)
                tamil_words.append(tamil)

data = pd.DataFrame({'French': french_words, 'Tamil': tamil_words})
print("Dataset Sample:\n", data.head())

# French tokenizer (char-level)
tokenizer_in = Tokenizer(char_level=True)
tokenizer_in.fit_on_texts(data['French'])
X_seq = tokenizer_in.texts_to_sequences(data['French'])
max_len_in = max(len(seq) for seq in X_seq)
X_pad = pad_sequences(X_seq, maxlen=max_len_in, padding='post')

# Tamil tokenizer (char-level)
tokenizer_out = Tokenizer(char_level=True)
tokenizer_out.fit_on_texts(data['Tamil'])
y_seq = tokenizer_out.texts_to_sequences(data['Tamil'])
max_len_out = max(len(seq) for seq in y_seq)
y_pad = pad_sequences(y_seq, maxlen=max_len_out, padding='post')

num_chars_in = len(tokenizer_in.word_index) + 1
num_chars_out = len(tokenizer_out.word_index) + 1

# One-hot encoding for output
y_onehot = np.array([to_categorical(seq, num_classes=num_chars_out) for seq in y_pad])

# Decoder input (shifted target for teacher forcing)
decoder_input = np.zeros_like(y_pad)
decoder_input[:, 1:] = y_pad[:, :-1]
decoder_input[:, 0] = 0  # start token index

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, TimeDistributed, RepeatVector

model = Sequential()
# Encoder
model.add(Embedding(input_dim=num_chars_in, output_dim=64))  # no input_length
model.add(LSTM(128))
# Repeat vector for decoder
model.add(RepeatVector(max_len_out))
# Decoder
model.add(LSTM(128, return_sequences=True))
model.add(TimeDistributed(Dense(num_chars_out, activation='softmax')))

# Compile
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

model.fit(X_pad, y_onehot, batch_size=32, epochs=1)
model.summary()  # This will now show correct parameter counts

history = model.fit(X_pad, y_onehot,
                    batch_size=32,
                    epochs=10,
                    validation_split=0.1)

pip install transformers torch sentencepiece

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
import torch

# Load pretrained mBART-50 model and tokenizer
model_name = "facebook/mbart-large-50-many-to-many-mmt"
model = MBartForConditionalGeneration.from_pretrained(model_name)
tokenizer = MBart50TokenizerFast.from_pretrained(model_name)

# Set source and target languages
src_lang = "fr_XX"  # French
tgt_lang = "ta_IN"  # Tamil

# Example French sentence
french_text = "Bonjour, comment ça va?"

# Tokenize input (keep batch dimension)
inputs = tokenizer(french_text, return_tensors="pt")

# Generate translation
translated_tokens = model.generate(
    **inputs,
    forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]
)

# Decode translated tokens
translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

print(f"French: {french_text}")
print(f"Tamil: {translated_text}")

test_words = ["apple", "table", "chair", "other"]

for word in test_words:
    inputs = tokenizer(word, return_tensors="pt")
    translated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id["ta_IN"]
    )
    tamil_word = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)
    print(f"French: {word} → Tamil: {tamil_word}")

!pip install gradio transformers torch --quiet

import gradio as gr
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

# Load pretrained model
model_name = "facebook/mbart-large-50-many-to-many-mmt"
model = MBartForConditionalGeneration.from_pretrained(model_name)
tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
tgt_lang = "ta_IN"

# Translation function
def translate_french(word):
    word = word.strip().lower()
    if len(word) != 5:
        return "Error: Only 5-letter French words are allowed!"

    inputs = tokenizer(word, return_tensors="pt")
    translated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]
    )
    tamil_word = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)
    return tamil_word

# Build Gradio interface
iface = gr.Interface(
    fn=translate_french,
    inputs=gr.Textbox(label="Enter 5-letter French word"),
    outputs=gr.Textbox(label="Tamil Translation"),
    title="French → Tamil Translator (5-letter words)",
    description="Enter a French word with exactly 5 letters to get its Tamil translation."
)

iface.launch()